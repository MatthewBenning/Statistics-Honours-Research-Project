{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multistep_LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPD0C/GFZoYzlENPiQrXNbF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewBenning/Statistics-Honours-Research-Project/blob/main/multistep_LSTM002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_MDT0DBVu6e"
      },
      "source": [
        "# LSTM for Multistep Forecasting \n",
        "# Author: Matthew Benning\n",
        "# Adapted from Eligijus Bujokas weather forecast framework with multivariate input\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import random \n",
        "import tensorflow as ts\n",
        "import statsmodels\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import coint, adfuller\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# Neccessary imports for LSTM, Keras package will be used\n",
        "from tensorflow import keras\n",
        "from keras.models import Input, Model, Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Concatenate, SimpleRNN, Masking, Flatten\n",
        "from keras import losses\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.initializers import RandomNormal\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "images_dir = '/content/gdrive/MyDrive/Statistics Honours Research Project'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stRI-kFtXyGR"
      },
      "source": [
        "\n",
        "# First read in the JSE data set and convert it to a timeseries object\n",
        "data = pd.read_excel('/content/gdrive/MyDrive/Statistics Honours Research Project/spread_series.xlsx',index_col=0)\n",
        "data = data.dropna()\n",
        "\n",
        "# Now we need to train each model individually, so we get series isolated\n",
        "data_1 = data.iloc[:,0]\n",
        "data_2 = data.iloc[:,1]\n",
        "data_3 = data.iloc[:,2]\n",
        "data_4 = data.iloc[:,3]\n",
        "data_5 = data.iloc[:,4]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzce7ZD6iBlD"
      },
      "source": [
        "#  Run cell multiple times for each pair, changing the parameters in a fixed manner\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# We can adjust these but this is the correct parameter specs for final model\n",
        "\n",
        "# Number of look back days to use\n",
        "# Steps ahead to forecast \n",
        "n_ahead = 7\n",
        "# S30% of the data is in the testing period\n",
        "test_share = 0.2\n",
        "# Epochs for training\n",
        "epochs = 50\n",
        "# Batch size \n",
        "batch_size = 500\n",
        "# Learning rate\n",
        "lr = 0.0001\n",
        "# Number of neurons in LSTM layer\n",
        "n_layer = 45\n",
        "\n",
        "# We use the past x days as input\n",
        "# Create the correct sequential data for LSTM X, Y series\n",
        "def form_XY(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n",
        "   \n",
        "    # Extracting the number of features that are passed from the array \n",
        "    n_features = 1\n",
        "\n",
        "    # Creating placeholder lists\n",
        "    X, Y = [], []\n",
        "\n",
        "    if len(ts) - lag <= 0:\n",
        "        X.append(ts)\n",
        "    else:\n",
        "        for i in range(len(ts) - lag - n_ahead):\n",
        "            Y.append(ts[(i + lag):(i + lag + n_ahead)])\n",
        "            X.append(ts[i:(i + lag)])\n",
        "\n",
        "    X, Y = np.array(X), np.array(Y)\n",
        "\n",
        "    # Reshaping the X array to an RNN input shape \n",
        "    X = np.reshape(X, (X.shape[0], lag, n_features))\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "# Run for our spread data\n",
        "X,Y = form_XY(data_1,lag=365,n_ahead=1,target_index=0)\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "############################################################\n",
        "\n",
        "# Set up the model class \n",
        "# We want an iteractive model training process so we include the \n",
        "# TrainCallback function which prevents over fitting once a certain mse reached\n",
        "\n",
        "class LSTM_spread_forecast():\n",
        "    \n",
        "    # Initialize our model class members\n",
        "    # Have kept the typescript set up as I just learnt it and its nice! MB :D\n",
        "    def __init__(\n",
        "        self, \n",
        "        X, \n",
        "        Y, \n",
        "        n_outputs,\n",
        "        n_lag,\n",
        "        n_ft,\n",
        "        n_layer,\n",
        "        batch,\n",
        "        epochs, \n",
        "        lr,\n",
        "        Xval=None,\n",
        "        Yval=None,\n",
        "        mask_value=-999.0,\n",
        "        min_delta=0.001,\n",
        "        patience=5\n",
        "    ):\n",
        "        lstm_input = Input(shape=(n_lag, 1))\n",
        "\n",
        "        \n",
        "        lstm_layer = LSTM(n_layer, activation='relu')(lstm_input)\n",
        "\n",
        "        x = Dense(n_outputs)(lstm_layer)\n",
        "        \n",
        "        self.model = Model(inputs=lstm_input, outputs=x)\n",
        "        self.batch = batch \n",
        "        self.epochs = epochs\n",
        "        self.n_layer=n_layer\n",
        "        self.lr = lr \n",
        "        self.Xval = Xval\n",
        "        self.Yval = Yval\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.mask_value = mask_value\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "\n",
        "    def trainCallback(self):\n",
        "        return EarlyStopping(monitor='loss', patience=self.patience, min_delta=self.min_delta)\n",
        "\n",
        "    def train(self):\n",
        "        # Getting the untrained model \n",
        "        empty_model = self.model\n",
        "        \n",
        "        # Init the optimizer\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "\n",
        "        # Compile the model\n",
        "        empty_model.compile(loss=losses.MeanAbsoluteError(), optimizer=optimizer)\n",
        "\n",
        "        if (self.Xval is not None) & (self.Yval is not None):\n",
        "            history = empty_model.fit(\n",
        "                self.X, \n",
        "                self.Y, \n",
        "                epochs=self.epochs, \n",
        "                batch_size=self.batch, \n",
        "                validation_data=(self.Xval, self.Yval), \n",
        "                shuffle=False,\n",
        "                callbacks=[self.trainCallback()]\n",
        "            )\n",
        "        else:\n",
        "            history = empty_model.fit(\n",
        "                self.X, \n",
        "                self.Y, \n",
        "                epochs=self.epochs, \n",
        "                batch_size=self.batch,\n",
        "                shuffle=False,\n",
        "                callbacks=[self.trainCallback()]\n",
        "            )\n",
        "        \n",
        "        # Saving to original model attribute in the class\n",
        "        self.model = empty_model\n",
        "        \n",
        "        # Returning the training history\n",
        "        return history\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "############################################################################\n",
        "# Now that model class is initiated we need to scale\n",
        "# Important that we scale the train and test seperately as to not introduce \n",
        "# any look back bias\n",
        "\n",
        "\n",
        "nrows = data_1.shape[0]\n",
        "# Spliting into train and test sets\n",
        "train = data_1[0:int(nrows * (1 -test_share))]\n",
        "test = data_1[int(nrows * (1 -test_share)):]\n",
        "# Scaling the data \n",
        "train_mean = train.mean()\n",
        "train_std = train.std()\n",
        "train = (train - train_mean) / train_std\n",
        "test = (test -train_mean) / train_std\n",
        "# Creating the final scaled frame \n",
        "ts_s = pd.concat([train, test])\n",
        "# Creating the X and Y for training\n",
        "X, Y = form_XY(ts_s.values, lag=lag, n_ahead=n_ahead)\n",
        "n_ft = 1\n",
        "\n",
        "########################################################################\n",
        "# Scaling done, now we can split the data into the respective subsets\n",
        "\n",
        "# Train and test splits\n",
        "Xtrain, Ytrain = X[0:int(X.shape[0] * (1 - test_share-0.10))], Y[0:int(X.shape[0] * (1 -test_share-0.10))]\n",
        "Xval, Yval = X[int(X.shape[0] * (1-test_share-0.10)):int(X.shape[0] * (1-test_share))], Y[int(X.shape[0] * (1 -test_share-0.10)):int(X.shape[0] * (1-test_share))]\n",
        "Xtest, Ytest = X[int(X.shape[0] * (1-test_share)):], Y[int(X.shape[0] * (1 -test_share)):]\n",
        "\n",
        "\n",
        "# Verify the final shapes, making sure correct set up for LSTM\n",
        "print(Xtrain.shape)\n",
        "print(Ytrain.shape)\n",
        "print(Xval.shape)\n",
        "print(Yval.shape)\n",
        "# Looks good to go!\n",
        "\n",
        "#####################################################################\n",
        "# Set up model class for the pair spread\n",
        "\n",
        "model_pair1 = LSTM_spread_forecast(\n",
        " X=Xtrain,\n",
        " Y=Ytrain,\n",
        " n_outputs=n_ahead,\n",
        " n_lag=lag,\n",
        " n_ft=n_ft,\n",
        " n_layer=n_layer,\n",
        " batch=batch_size,\n",
        " epochs=epochs, \n",
        " lr=lr,\n",
        " Xval=Xval,\n",
        " Yval=Yval,\n",
        ")\n",
        "# Check the model summary before running\n",
        "model_pair1.model.summary()\n",
        "# Training of the model \n",
        "history = model_pair1.train()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfFHc1owUvI0",
        "outputId": "49b02772-be82-4700-e684-d41e98e2ae64"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<keras.callbacks.History object at 0x7f238604fc10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wV9D1bSrSYuP"
      },
      "source": [
        "# Tried to run a loop but was a bit too messy to keep track of, so just\n",
        "# changed parameters individually in code above for each pair\n",
        "\n",
        "# Dont run\n",
        "\n",
        "for i in range(10):\n",
        "  for j in range (12):\n",
        "\n",
        "    # Number of look back days to use\n",
        "    lag = 10*(1+i)\n",
        "    # Steps ahead to forecast \n",
        "    n_ahead = 7\n",
        "    # S30% of the data is in the testing period\n",
        "    test_share = 0.2\n",
        "    # Epochs for training\n",
        "    epochs = 100\n",
        "    # Batch size \n",
        "    batch_size = 500\n",
        "    # Learning rate\n",
        "    lr = 0.0001\n",
        "    # Number of neurons in LSTM layer\n",
        "    n_layer = 5*(1+j)\n",
        "\n",
        "    # We use the past x days as input\n",
        "    # Create the correct sequential data for LSTM X, Y series\n",
        "    def form_XY(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n",
        "      \n",
        "        # Extracting the number of features that are passed from the array \n",
        "        n_features = 1\n",
        "\n",
        "        # Creating placeholder lists\n",
        "        X, Y = [], []\n",
        "\n",
        "        if len(ts) - lag <= 0:\n",
        "            X.append(ts)\n",
        "        else:\n",
        "            for i in range(len(ts) - lag - n_ahead):\n",
        "                Y.append(ts[(i + lag):(i + lag + n_ahead)])\n",
        "                X.append(ts[i:(i + lag)])\n",
        "\n",
        "        X, Y = np.array(X), np.array(Y)\n",
        "\n",
        "        # Reshaping the X array to an RNN input shape \n",
        "        X = np.reshape(X, (X.shape[0], lag, n_features))\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Run for our spread data\n",
        "    X,Y = form_XY(data_1,lag=365,n_ahead=1,target_index=0)\n",
        "\n",
        "    print(X.shape)\n",
        "    print(Y.shape)\n",
        "\n",
        "    ############################################################\n",
        "\n",
        "    # Set up the model class \n",
        "    # We want an iteractive model training process so we include the \n",
        "    # TrainCallback function which prevents over fitting once a certain mse reached\n",
        "\n",
        "    class LSTM_spread_forecast():\n",
        "        \n",
        "        # Initialize our model class members\n",
        "        # Have kept the typescript set up as I just learnt it and its nice! MB :D\n",
        "        def __init__(\n",
        "            self, \n",
        "            X, \n",
        "            Y, \n",
        "            n_outputs,\n",
        "            n_lag,\n",
        "            n_ft,\n",
        "            n_layer,\n",
        "            batch,\n",
        "            epochs, \n",
        "            lr,\n",
        "            Xval=None,\n",
        "            Yval=None,\n",
        "            mask_value=-999.0,\n",
        "            min_delta=0.001,\n",
        "            patience=5\n",
        "        ):\n",
        "            lstm_input = Input(shape=(n_lag, 1))\n",
        "\n",
        "            \n",
        "            lstm_layer = LSTM(n_layer, activation='relu')(lstm_input)\n",
        "\n",
        "            x = Dense(n_outputs)(lstm_layer)\n",
        "            \n",
        "            self.model = Model(inputs=lstm_input, outputs=x)\n",
        "            self.batch = batch \n",
        "            self.epochs = epochs\n",
        "            self.n_layer=n_layer\n",
        "            self.lr = lr \n",
        "            self.Xval = Xval\n",
        "            self.Yval = Yval\n",
        "            self.X = X\n",
        "            self.Y = Y\n",
        "            self.mask_value = mask_value\n",
        "            self.min_delta = min_delta\n",
        "            self.patience = patience\n",
        "\n",
        "        def trainCallback(self):\n",
        "            return EarlyStopping(monitor='loss', patience=self.patience, min_delta=self.min_delta)\n",
        "\n",
        "        def train(self):\n",
        "            # Getting the untrained model \n",
        "            empty_model = self.model\n",
        "            \n",
        "            # Init the optimizer\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "\n",
        "            # Compile the model\n",
        "            empty_model.compile(loss=losses.MeanAbsoluteError(), optimizer=optimizer)\n",
        "\n",
        "            if (self.Xval is not None) & (self.Yval is not None):\n",
        "                history = empty_model.fit(\n",
        "                    self.X, \n",
        "                    self.Y, \n",
        "                    epochs=self.epochs, \n",
        "                    batch_size=self.batch, \n",
        "                    validation_data=(self.Xval, self.Yval), \n",
        "                    shuffle=False,\n",
        "                    callbacks=[self.trainCallback()]\n",
        "                )\n",
        "            else:\n",
        "                history = empty_model.fit(\n",
        "                    self.X, \n",
        "                    self.Y, \n",
        "                    epochs=self.epochs, \n",
        "                    batch_size=self.batch,\n",
        "                    shuffle=False,\n",
        "                    callbacks=[self.trainCallback()]\n",
        "                )\n",
        "            \n",
        "            # Saving to original model attribute in the class\n",
        "            self.model = empty_model\n",
        "            \n",
        "            # Returning the training history\n",
        "            return history\n",
        "        \n",
        "        def predict(self, X):\n",
        "            return self.model.predict(X)\n",
        "\n",
        "    ############################################################################\n",
        "    # Now that model class is initiated we need to scale\n",
        "    # Important that we scale the train and test seperately as to not introduce \n",
        "    # any look back bias\n",
        "\n",
        "\n",
        "    nrows = data_1.shape[0]\n",
        "    # Spliting into train and test sets\n",
        "    train = data_1[0:int(nrows * (1 -test_share))]\n",
        "    test = data_1[int(nrows * (1 -test_share)):]\n",
        "    # Scaling the data \n",
        "    train_mean = train.mean()\n",
        "    train_std = train.std()\n",
        "    train = (train - train_mean) / train_std\n",
        "    test = (test -train_mean) / train_std\n",
        "    # Creating the final scaled frame \n",
        "    ts_s = pd.concat([train, test])\n",
        "    # Creating the X and Y for training\n",
        "    X, Y = form_XY(ts_s.values, lag=lag, n_ahead=n_ahead)\n",
        "    n_ft = 1\n",
        "\n",
        "    ########################################################################\n",
        "    # Scaling done, now we can split the data into the respective subsets\n",
        "\n",
        "    # Train and test splits\n",
        "    Xtrain, Ytrain = X[0:int(X.shape[0] * (1 - test_share-0.10))], Y[0:int(X.shape[0] * (1 -test_share-0.10))]\n",
        "    Xval, Yval = X[int(X.shape[0] * (1-test_share-0.1)):int(X.shape[0] * (1-test_share))], Y[int(X.shape[0] * (1 -test_share-0.1)):int(X.shape[0] * (1-test_share))]\n",
        "    Xtest, Ytest = X[int(X.shape[0] * (1-test_share)):], Y[int(X.shape[0] * (1 -test_share)):]\n",
        "\n",
        "\n",
        "    # Verify the final shapes, making sure correct set up for LSTM\n",
        "    #print(Xtrain.shape)\n",
        "    #print(Ytrain.shape)\n",
        "    #print(Xval.shape)\n",
        "    #print(Yval.shape)\n",
        "    # Looks good to go!\n",
        "\n",
        "    #####################################################################\n",
        "    # Set up model class for the pair spread\n",
        "\n",
        "    model_pair1 = LSTM_spread_forecast(\n",
        "    X=Xtrain,\n",
        "    Y=Ytrain,\n",
        "    n_outputs=n_ahead,\n",
        "    n_lag=lag,\n",
        "    n_ft=n_ft,\n",
        "    n_layer=n_layer,\n",
        "    batch=batch_size,\n",
        "    epochs=epochs, \n",
        "    lr=lr,\n",
        "    Xval=Xval,\n",
        "    Yval=Yval,\n",
        "    )\n",
        "    # Check the model summary before running\n",
        "    model_pair1.model.summary()\n",
        "    # Training of the model \n",
        "    history = model_pair1.train()\n",
        "    cp = ModelCheckpoint('model_pair1', save_best_only = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKFH3_vOsnP7"
      },
      "source": [
        "# After model has trained we can now check the results and plot some curves\n",
        "\n",
        "# Extract the training loss and validation loss\n",
        "loss = history.history.get('loss')\n",
        "val_loss = history.history.get('val_loss')\n",
        "\n",
        "# Get the number of epochs that was run before cutting off\n",
        "n_epochs = range(len(loss))\n",
        "\n",
        "# Plot the training loss and val loss\n",
        "plt.figure(figsize=(9, 7))\n",
        "plt.plot(n_epochs, loss, 'r', label='Training loss', color='blue')\n",
        "if val_loss is not None:\n",
        "    plt.plot(n_epochs, val_loss, 'r', label='Validation loss', color='red')\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()\n",
        "\n",
        "# Forecasts against the actual values\n",
        "forecast = model_pair1.predict(Xval)\n",
        "\n",
        "# Plot the forecasts\n",
        "yhat = forecast[1:751,]*train_std +train_mean\n",
        "y = Yval*train_std +train_mean\n",
        "\n",
        "plt.plot(yhat,color='red')\n",
        "plt.plot(y,color='blue')\n",
        "plt.show()\n",
        "\n",
        "# Test period \n",
        "test_forecast_1= model_pair1.predict(Xtest)\n",
        "test_forecast_1 = test_forecast_1*test_std+test_mean\n",
        "plot(test_forecast_1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}